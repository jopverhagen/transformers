{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "439e513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.,  1., 22.]])\n",
      "tensor([[-1.5000,  0.5000, 11.0000]])\n",
      "tensor([[0., 0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10.0000,  9.9997]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# (3, 2)\n",
    "keys = torch.Tensor([ [1,0], #ik key\n",
    "                      [0,1], #ben key\n",
    "                      [-7,1]  #blij key\n",
    "                    ])\n",
    "\n",
    "\n",
    "# (3, 2)\n",
    "values = torch.Tensor([[ 1,0], #ik value\n",
    "                       [10,0], #ben value\n",
    "                       [10,10] #blij value\n",
    "                      ]) \n",
    "\n",
    "# (1, 2)\n",
    "query = torch.Tensor([[-3,1]]) #ik\n",
    "\n",
    "print(torch.matmul(query, keys.T))\n",
    "qk_scaled = torch.matmul(query, keys.T) / math.sqrt(4)\n",
    "\n",
    "print(qk_scaled )\n",
    "\n",
    "attention = F.softmax(qk_scaled, dim=-1)\n",
    "print(np.round(attention,decimals=2))\n",
    "\n",
    "output = torch.matmul(attention, values)\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5f8a2e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38, 0.23, 0.38]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.Tensor([[1, 0]]) #ben\n",
    "QK = torch.matmul(query, keys.T)\n",
    "    \n",
    "##scale QK by the sqrt of dk\n",
    "matmul_scaled = QK / math.sqrt(4)\n",
    "F.softmax(matmul_scaled, dim=-1)\n",
    "\n",
    "softmax_output = F.softmax(matmul_scaled, dim=-1)\n",
    "\n",
    "# Converteer de tensor naar een numpy array\n",
    "softmax_numpy = softmax_output.detach().numpy()\n",
    "\n",
    "# Rond de waardes af\n",
    "rounded_values = np.round(softmax_numpy, decimals=2)\n",
    "rounded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "693fbc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0.]])\n",
      "Attention weights are:\n",
      "[[0. 1. 0. 0.]]\n",
      "\n",
      "Output is:\n",
      "[[10.  0.  2.]]\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000]])\n",
      "Attention weights are:\n",
      "[[0.  0.  0.5 0.5]]\n",
      "\n",
      "Output is:\n",
      "[[550.    5.5   0. ]]\n",
      "tensor([[0., 1., 0., 0.]])\n",
      "Attention weights are:\n",
      "[[0. 1. 0. 0.]]\n",
      "\n",
      "Output is:\n",
      "[[10.  0.  2.]]\n",
      "tensor([[0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000]])\n",
      "Attention weights are:\n",
      "[[0.  1.  0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]]\n",
      "\n",
      "Output is:\n",
      "[[ 10.    0.    2. ]\n",
      " [550.    5.5   0. ]\n",
      " [ 10.    0.    2. ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scaled_dot_product_attention(Q, K, V, dk=4):\n",
    "    ##matmul Q and K\n",
    "    QK = torch.matmul(Q, K.T)\n",
    "    \n",
    "    ##scale QK by the sqrt of dk\n",
    "    matmul_scaled = QK / math.sqrt(dk)\n",
    "    \n",
    "    attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
    "    print(np.round(attention_weights,decimals=2))\n",
    "\n",
    "    ## matmul attention_weights by V\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# %%\n",
    "def print_attention(Q, K, V, n_digits = 3):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(Q, K, V)\n",
    "    temp_out, temp_attn = temp_out.numpy(), temp_attn.numpy()\n",
    "    print ('Attention weights are:')\n",
    "    print (np.round(temp_attn, n_digits))\n",
    "    print()\n",
    "    print ('Output is:')\n",
    "    print (np.around(temp_out, n_digits))\n",
    "\n",
    "# %%\n",
    "temp_k = torch.Tensor([[10,0,0],\n",
    "                      [0,20,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]])  # (4, 3)\n",
    "\n",
    "temp_v = torch.Tensor([[   1,0, 1],\n",
    "                      [  10,0, 2],\n",
    "                      [ 100,5, 0],\n",
    "                      [1000,6, 0]])  # (4, 3)\n",
    "\n",
    "# %%\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = torch.Tensor([[0, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)\n",
    "\n",
    "# %%\n",
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = torch.Tensor([[0, 0, 10]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)\n",
    "\n",
    "# %%\n",
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = torch.Tensor([[10, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)\n",
    "\n",
    "# %%\n",
    "temp_q = torch.Tensor([[0, 10, 0], [0, 0, 10], [10, 10, 0]])  # (3, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
